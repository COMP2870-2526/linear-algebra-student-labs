{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263f1360-e304-42ae-bd18-7faa4c796b79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lab05: Improving QR factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a8ea0-041f-48db-91b6-2ed6eff92c95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dde3cb-aaa1-4e14-ad9d-a2965243faca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: Problems with QR factorisation\n",
    "\n",
    "The Gram-Schmidt process is known to not be numerically stable. In practice, this means that the method can have problems which lead to loss of orthogonality through the process. In this exercise, we will explore this problem on a simple $2 \\times 2$ problem.\n",
    "\n",
    "For $\\varepsilon > 0$, let $A_\\varepsilon$ be the matrix given by\n",
    "$$\n",
    "A_\\varepsilon = \\begin{pmatrix}\n",
    "1 & 1 + \\varepsilon \\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "There are three ways to judge the accuracy of a QR factorisation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{error}_1 &= \\| A - QR \\|_2 \\\\\n",
    "\\text{error}_2 &= \\| Q^T Q - I_{n} \\|_2 \\\\\n",
    "\\text{error}_3 &= \\| R - \\text{toupper}(R) \\|_2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\| . \\|_2$ is a norm on matrices than can be computed using `np.linalg.norm(...)` and $\\text{toupper}$ is a function that takes a matrix to only its upper trianglar part which can be computer using `np.triu(...)`.\n",
    "\n",
    "1. Create a table with the columns $\\varepsilon$, $\\text{error}_1$, $\\text{error}_2$ and $\\text{error}_3$ for $\\varepsilon = 10^{-6}, 10^{-7}, \\ldots, 10^{-16}$ using the classical Gram-Scmidt process from the lectures.\n",
    "2. Explain why you see the results are not as good as you would like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0389b52-7e9f-48dd-a19d-045ff19d08c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Portfolio exercise</b>\n",
    "You should submit:\n",
    "\n",
    "1. The code to generate the table.\n",
    "2. The tables of results.\n",
    "3. Your explanation of the results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930ee61-5586-4831-9c7c-1f205fe5e52d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 2: Improving the QR with Gram Schmidt approach\n",
    "\n",
    "In this exercise, we will improve our implementation of\n",
    "`gram_schmidt_eigen`.\n",
    "\n",
    "Implement *one* of the following improvements and see how the change the\n",
    "resulting performance of the algorithm in terms of robustness, accuracy\n",
    "and/or efficiency.\n",
    "\n",
    "1.  Following [Example\n",
    "    7.3](https://comp2870-2526.github.io/linear-algebra-notes/src/lec07.html#exm-char),\n",
    "    we have a good formula for how *shifting* a matrix changes the\n",
    "    eigenvalues and eigenvectors of a matrix.\n",
    "\n",
    "    For a shift value $\\mu$, the key idea for the shifted algorithm is\n",
    "    to shift the matrix before computing QR factorisation $$\n",
    "     A - \\mu I_n = QR,\n",
    "     $$ and then undo the shift when updating $$\n",
    "     A^{\\text{new}} = RQ + \\mu I_n.\n",
    "     $$\n",
    "\n",
    "    There are two suggestions form the literature to improve the speed\n",
    "    of convergence.\n",
    "\n",
    "    -   The first is to take the bottom right value of the matrix $A$ at\n",
    "        each step $\\mu = A_{nn}$. Note this value changes throughout the\n",
    "        computation as we update $A$ in-place.\n",
    "    -   The second is to use the *Wilkinson shift*. In this case, we\n",
    "        compute the eigenvalues of the bottom-right $2 \\times 2$\n",
    "        submatrix of $A$ (using your code from Lab04) and then\n",
    "        choose the eigenvalue closest to the bottom-right element of $A$\n",
    "        as our value of $\\mu$.\n",
    "\n",
    "    Neither change affects how the matrix $V$ should be computed.\n",
    "\n",
    "2.  We can also scale the matrix to be close to unit size. We again do\n",
    "    this before the QR factorisation and then undo during reconstruction\n",
    "\n",
    "    1.  Set $\\alpha = \\max_{ij} | A_{ij} |$ and\n",
    "        $\\tilde{A} = A / \\alpha$.\n",
    "    2.  Compute the QR factorisation of $\\tilde{A} = QR$.\n",
    "    3.  Update the $A$ as $$A^{\\text{new}} = \\alpha RQ.$$\n",
    "\n",
    "    This change does not affect how the matrix $V$ should be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b78ae0-9f72-4832-94ca-d26f0b0451ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test your solution by finding the eigenvalues and eigenvectors of 10 randomly created symmetric matrices of sizes $n = 2, 4, 8, 16, 32$. Comment on what you find.\n",
    "\n",
    "For the method for the lectures and each approach you implement, make a table of problem size, average (mean) time taken, average QR iterations and maximum error as implemented by the `test_accuracy_of_eigensolve` function.\n",
    "\n",
    "How could you improve the variation of the method further in order to improve the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfa112-a3c2-472c-8b3e-000ee96f300c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "# replicable random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def random_symmetric_matrix(n):\n",
    "    # generate a random matrix\n",
    "    S = special_ortho_group.rvs(n)\n",
    "    D = np.diag(np.random.randint(1, 10, (n,)) / 2)\n",
    "    A = S.T @ D @ S\n",
    "    return A\n",
    "\n",
    "\n",
    "def test_accuracy_of_eigensolve(A, eigenvalues, eigenvectors):\n",
    "    \"\"\"\n",
    "    test accuracy of solution of eigenvalue problem\n",
    "    \"\"\"\n",
    "    residuals = []\n",
    "    for i in range(len(eigenvalues)):\n",
    "        residual = np.linalg.norm(\n",
    "            A @ eigenvectors[:, i] - eigenvalues[i] * eigenvectors[:, i]\n",
    "        )\n",
    "        residuals.append(residual)\n",
    "    return max(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05180e89-fd0d-49af-acee-2085acee3a4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If you have time, implement one or more additional updates again\n",
    "comparing using the test cases from the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ced9bf-2d19-4265-8962-8e43151bf3ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
